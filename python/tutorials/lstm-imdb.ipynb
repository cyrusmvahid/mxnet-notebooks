{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Solution\n",
    "## Step 1: Create dictionary\n",
    "- Load the vocab file into numpy using readlines\n",
    "## Step 2: Create paragraph vectors\n",
    "- Load all the trainign and test data into an array. This will create 4x12500 length arrays.\n",
    " - train/pos\n",
    " - train/neg\n",
    " - test/pos\n",
    " - test/neg\n",
    " each of these arays in fact includes a paragraph\n",
    "## Step 3: Cteate BOW\n",
    "- tokenize each element in each array. This shanges shape of the vectors. Initially vector shapes are (12500, 1). \n",
    "- do wordcount (word, count)\n",
    "- get rid of most common words\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and env-variable\n",
    "import numpy as mp\n",
    "import os\n",
    "import glob\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "\n",
    "BASE_DIR = '/home/ubuntu/CyrusProjects/mxnet-notebooks/python/tutorials'\n",
    "PATH='/aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89527\n"
     ]
    }
   ],
   "source": [
    "with open(BASE_DIR + PATH + '/imdb.vocab') as f:\n",
    "    dic = f.readlines()\n",
    "dic= [x.strip() for x in dic]\n",
    "dic = np.array(np.sort(dic))\n",
    "dic = np.unique(dic)\n",
    "dic = dic.tolist()\n",
    "print(len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "arr = [1,2,3,4,5]\n",
    "try:\n",
    "    print(arr.index(3))\n",
    "except ValueError:\n",
    "    print(\"item does not exist\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tokenizer.perl is from Moses: https://github.com/moses-smt/mosesdecoder/tree/master/scripts/tokenizer\n",
    "def sentence_array_creator(path, root_dir):\n",
    "    os.chdir(path)\n",
    "    sentences = []\n",
    "    for file in list(glob.glob(\"*.txt\")):\n",
    "        with open(file, 'r') as f:\n",
    "            sentences.append(f.readline().strip().lower())\n",
    "    return sentences\n",
    "    os.chdir(root_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method format of str object at 0x7f5d2cc0ca98>\n",
      "12500, 12500, 12500, 12500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(BASE_DIR)\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "base_dir = root_dir + path\n",
    "pos_train_path = base_dir + '/train/pos'\n",
    "neg_train_path = base_dir + '/train/neg'\n",
    "pos_test_path = base_dir + '/test/pos'\n",
    "neg_test_path = base_dir + '/test/neg'\n",
    "\n",
    "\n",
    "pos_train_sentences = sentence_array_creator(pos_train_path, root_dir)\n",
    "neg_train_sentences = sentence_array_creator(neg_train_path, root_dir)\n",
    "pos_test_sentences = sentence_array_creator(pos_test_path, root_dir)\n",
    "neg_test_sentences = sentence_array_creator(neg_test_path, root_dir)\n",
    "print('{}; {}; {}; {}; '.format)\n",
    "\n",
    "print(\"{}, {}, {}, {}\".format(len(pos_train_sentences), len(neg_train_sentences), len(pos_test_sentences), len(neg_test_sentences)))\n",
    "\n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "def get_dic_index(word, dic):\n",
    "    try:\n",
    "        ret_val = dic.index(word)\n",
    "    except ValueError: \n",
    "        ret_val = -1\n",
    "    return ret_val\n",
    "\n",
    "    \n",
    "word = \"aamir\"\n",
    "print(get_dic_index(word, dic))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sentence_embedding(sentences):\n",
    "    t = MosesTokenizer()\n",
    "    bow = []\n",
    "    for i in range(len(sentences)):\n",
    "        elements = []\n",
    "        tokens = t.tokenize(pos_test_sentences[i])\n",
    "        for token in tokens:\n",
    "            index = get_dic_index(token, dic)\n",
    "            if index != -1:\n",
    "                elements.append(index)\n",
    "        bow.append(elements)\n",
    "    return bow\n",
    "    \n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bow_pos_train_sentences = sentence_embedding(pos_train_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[23842, 4161, 39813, 64192, 58688, 10892, 29669, 35143, 52601, 17451, 68872, 2614, 4201, 28777, 79025, 61359, 39813, 49020, 20243, 88728, 1338, 9147, 9147, 79344, 27653, 68359, 48180, 71592, 5255, 38082, 87003, 39920, 1495, 55160, 69303, 39920, 1252, 88728, 43533, 39920, 75241, 88101, 9147, 9147, 79025, 51028, 39813, 31, 44960, 72491, 2614, 60829, 79025, 18644, 3728, 84937, 77941, 2614, 79158, 31, 77707, 34067, 24322, 10492, 79344, 39813, 31, 81915, 51028, 72991, 88923, 34482, 80083, 31383, 39920, 23842, 4161, 39813, 25712, 6862, 30648, 39813, 54998, 2614, 39920, 29879, 80083, 69287, 49020, 21992, 72991, 2614, 79405, 78534, 10892, 75241, 68354, 79025, 45010, 18491, 55925, 54754, 37329, 721, 33439, 80083, 81827, 31, 44574, 55160, 22853, 55234, 54754, 79073, 0, 9147, 9147, 31, 17007, 44960, 81915, 88193, 69303]\n"
     ]
    }
   ],
   "source": [
    "print(len(bow_pos_train_sentences))\n",
    "print(bow_pos_train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'seems', 'Hal', 'Hartley', '&apos;s', 'films', 'are', 'kind', 'of', 'hit', 'or', 'miss', 'with', 'most', 'audiences', '.', 'This', 'film', 'will', 'be', 'no', 'exception', 'to', 'that', 'rule', '.', 'Fay', 'Grim', 'acts', 'as', 'a', 'sequel', 'to', 'Hartley', '&apos;s', '&apos;', 'Henry', 'Foole', '&apos;', 'from', '1998', '.', 'The', 'focus', 'this', 'time', 'is', 'on', 'Henry', '&apos;s', 'ex', 'wife', '(', 'played', 'to', 'perfection', 'by', 'the', 'always', 'welcome', 'Parker', 'Posey', ')', ',', 'who', 'is', 'being', 'pestered', 'by', 'CIA', 'goons', 'about', 'Henry', '&apos;s', 'unpublished', 'book', 'about', 'all', 'of', 'his', 'shady', 'dealings', '.', 'In', 'the', 'interim', 'of', 'all', 'of', 'this', ',', 'Fay', 'ends', 'up', 'on', 'an', 'odyssey', ',', 'dealing', 'with', 'international', 'spies', ',', 'etc', '.', 'The', 'film', 'does', 'get', 'a', 'bit', 'bogged', 'down', 'in', 'the', 'second', 'half', '.', 'If', 'you', '&apos;ve', 'been', 'a', 'fan', 'of', 'Hal', 'Hartley', 'in', 'the', 'past', ',', 'this', 'is', 'one', 'not', 'to', 'be', 'missed', '.', 'For', 'the', 'novice', 'Hartley', 'first', 'timer', 'who', 'has', 'only', 'heard', 'of', 'his', 'film', 'making', 'technique', ',', 'you', 'might', 'want', 'to', 'check', 'out', 'his', 'earlier', 'films', 'before', 'taking', 'on', 'this', 'one', '(', 'especially', 'if', 'you', 'haven', '&apos;t', 'seen', '&apos;', 'Henry', '&apos;', 'yet', ')', '.', 'I', 'admired', 'the', 'camera', 'work', ',', 'which', 'at', 'times', 'reminded', 'me', 'of', 'certain', 'early', 'Man', 'Ray', 'photography', '.']\n",
      "----------------------------------------\n",
      "['&apos;' '&apos;s' '&apos;t' '&apos;ve' '(' ')' ',' '.' '1998' 'CIA' 'Fay'\n",
      " 'Foole' 'For' 'Grim' 'Hal' 'Hartley' 'Henry' 'I' 'If' 'In' 'It' 'Man'\n",
      " 'Parker' 'Posey' 'Ray' 'The' 'This' 'a' 'about' 'acts' 'admired' 'all'\n",
      " 'always' 'an' 'are' 'as' 'at' 'audiences' 'be' 'been' 'before' 'being'\n",
      " 'bit' 'bogged' 'book' 'by' 'camera' 'certain' 'check' 'dealing' 'dealings'\n",
      " 'does' 'down' 'earlier' 'early' 'ends' 'especially' 'etc' 'ex' 'exception'\n",
      " 'fan' 'film' 'films' 'first' 'focus' 'from' 'get' 'goons' 'half' 'has'\n",
      " 'haven' 'heard' 'his' 'hit' 'if' 'in' 'interim' 'international' 'is'\n",
      " 'kind' 'making' 'me' 'might' 'miss' 'missed' 'most' 'no' 'not' 'novice'\n",
      " 'odyssey' 'of' 'on' 'one' 'only' 'or' 'out' 'past' 'perfection' 'pestered'\n",
      " 'photography' 'played' 'reminded' 'rule' 'second' 'seems' 'seen' 'sequel'\n",
      " 'shady' 'spies' 'taking' 'technique' 'that' 'the' 'this' 'time' 'timer'\n",
      " 'times' 'to' 'unpublished' 'up' 'want' 'welcome' 'which' 'who' 'wife'\n",
      " 'will' 'with' 'work' 'yet' 'you']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.moses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "\n",
    "t =  MosesTokenizer()\n",
    "tokens = t.tokenize(sentences[0])\n",
    "print(tokens)\n",
    "print('----------------------------------------')\n",
    "print(np.unique(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package hmm_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package hmm_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "os.system('pip3 install nltk')\n",
    "nltk.download('all')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "http = urllib3.PoolManager()\n",
    "import os\n",
    "\n",
    "\n",
    "url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "path = os.getcwd() + '/local_aclImdb_v1.tar.gz'\n",
    "chunk_size = 1024\n",
    "\n",
    "r = http.request('GET', url, preload_content=False)\n",
    "\n",
    "with open(path, 'wb') as out:\n",
    "    while True:\n",
    "        data = r.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        out.write(data)\n",
    "\n",
    "r.release_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89527\n"
     ]
    }
   ],
   "source": [
    "with open('aclImdb/imdb.vocab') as vocab_file:\n",
    "    vocab_array = vocab_file.readlines()\n",
    "vocab_array = [x.strip() for x in vocab_array]\n",
    "print(len(vocab_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StringIO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2f9fc64d9ffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"M 21 72\\nF 35 58\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'formats'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'S1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StringIO' is not defined"
     ]
    }
   ],
   "source": [
    "d = StringIO(\"M 21 72\\nF 35 58\")\n",
    "np.loadtxt(d, dtype={'names': ('gender', 'age', 'weight'),'formats': ('S1', 'i4', 'f4')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_dict(path):\n",
    "    sentences = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir('%s/pos/' % path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    os.chdir('%s/neg/' % path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    os.chdir(currdir)\n",
    "\n",
    "    sentences = tokenize(sentences)\n",
    "\n",
    "    print('Building dictionary..', end=' ')\n",
    "    wordcount = dict()\n",
    "    for ss in sentences:\n",
    "        words = ss.strip().lower().split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = wordcount.values()\n",
    "    keys = wordcount.keys()\n",
    "\n",
    "    sorted_idx = numpy.argsort(counts)[::-1]\n",
    "\n",
    "    worddict = dict()\n",
    "\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "\n",
    "    print(numpy.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "\n",
    "    return worddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'ain', '&apos;t', 'funny', '.', 'It', '&apos;s', 'actually', 'hillarious', ',', 'yet', 'double', 'Ls', '.', '&#124;', '&#91;', '&#93;', '&lt;', '&gt;', '&#91;', '&#93;', '&amp;', 'You', '&apos;re', 'gonna', 'shake', 'it', 'off', '?', 'Don', '&apos;t', '?']\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.moses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "\n",
    "text = u'This, is a sentence with weird» symbols… appearing everywhere¿'\n",
    "t, d = MosesTokenizer(), MosesDetokenizer()\n",
    "sent = \"This ain't funny. It's actually hillarious, yet double Ls. | [] < > [ ] & You're gonna shake it off? Don't?\"\n",
    "expected_tokens = [u'This', u'ain', u'&apos;t', u'funny', u'.', u'It', u'&apos;s', u'actually', u'hillarious', u',', u'yet', u'double', u'Ls', u'.', u'&#124;', u'&#91;', u'&#93;', u'&lt;', u'&gt;', u'&#91;', u'&#93;', u'&amp;', u'You', u'&apos;re', u'gonna', u'shake', u'it', u'off', u'?', u'Don', u'&apos;t', u'?']\n",
    "expected_detokens = \"This ain't funny. It's actually hillarious, yet double Ls. | [] < > [] & You're gonna shake it off? Don't?\"\n",
    "tokens = t.tokenize(sent)\n",
    "print(tokens)\n",
    "if tokens == expected_tokens:\n",
    "    print(\"True\")\n",
    "detokens = d.detokenize(tokens)\n",
    "if \" \".join(detokens) == expected_detokens:\n",
    "    print(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f579577a6fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "\"\"\" Referemce: http://deeplearning.net/tutorial/code/imdb_preprocess.py\"\"\"\n",
    "\"\"\"\n",
    "This script is what created the dataset pickled.\n",
    "\n",
    "1) You need to download this file and put it in the same directory as this file.\n",
    "https://github.com/moses-smt/mosesdecoder/raw/master/scripts/tokenizer/tokenizer.perl . Give it execution permission.\n",
    "\n",
    "2) Get the dataset from http://ai.stanford.edu/~amaas/data/sentiment/ and extract it in the current directory.\n",
    "\n",
    "3) Then run this script.\n",
    "\"\"\"\n",
    "\n",
    "dataset_path='aclImdb_v1.tar.gz'\n",
    "\n",
    "import numpy\n",
    "import _pickle as pkl\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "# tokenizer.perl is from Moses: https://github.com/moses-smt/mosesdecoder/tree/master/scripts/tokenizer\n",
    "tokenizer_cmd = ['./tokenizer.perl', '-l', 'en', '-q', '-']\n",
    "\n",
    "\n",
    "def tokenize(sentences):\n",
    "\n",
    "    print('Tokenizing..', end=' ')\n",
    "    text = \"\\n\".join(sentences)\n",
    "    tokenizer = Popen(tokenizer_cmd, stdin=PIPE, stdout=PIPE)\n",
    "    tok_text, _ = tokenizer.communicate(text)\n",
    "    toks = tok_text.split('\\n')[:-1]\n",
    "    print('Done')\n",
    "\n",
    "    return toks\n",
    "\n",
    "\n",
    "def build_dict(path):\n",
    "    sentences = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir('%s/pos/' % path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    os.chdir('%s/neg/' % path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    os.chdir(currdir)\n",
    "\n",
    "    sentences = tokenize(sentences)\n",
    "\n",
    "    print('Building dictionary..', end=' ')\n",
    "    wordcount = dict()\n",
    "    for ss in sentences:\n",
    "        words = ss.strip().lower().split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = wordcount.values()\n",
    "    keys = wordcount.keys()\n",
    "\n",
    "    sorted_idx = numpy.argsort(counts)[::-1]\n",
    "\n",
    "    worddict = dict()\n",
    "\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "\n",
    "    print(numpy.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "\n",
    "    return worddict\n",
    "\n",
    "\n",
    "def grab_data(path, dictionary):\n",
    "    sentences = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    os.chdir(currdir)\n",
    "    sentences = tokenize(sentences)\n",
    "\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        words = ss.strip().lower().split()\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in words]\n",
    "\n",
    "    return seqs\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Get the dataset from http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "    path = dataset_path\n",
    "    dictionary = build_dict(os.path.join(path, 'train'))\n",
    "\n",
    "    train_x_pos = grab_data(path+'train/pos', dictionary)\n",
    "    train_x_neg = grab_data(path+'train/neg', dictionary)\n",
    "    train_x = train_x_pos + train_x_neg\n",
    "    train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "    test_x_pos = grab_data(path+'test/pos', dictionary)\n",
    "    test_x_neg = grab_data(path+'test/neg', dictionary)\n",
    "    test_x = test_x_pos + test_x_neg\n",
    "    test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "    f = open('imdb.pkl', 'wb')\n",
    "    pkl.dump((train_x, train_y), f, -1)\n",
    "    pkl.dump((test_x, test_y), f, -1)\n",
    "    f.close()\n",
    "\n",
    "    f = open('imdb.dict.pkl', 'wb')\n",
    "    pkl.dump(dictionary, f, -1)\n",
    "    f.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
