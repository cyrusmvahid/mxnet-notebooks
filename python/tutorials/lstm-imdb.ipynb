{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Solution\n",
    "## Step 1: Create dictionary\n",
    "- Load the vocab file into numpy using readlines\n",
    "## Step 2: Create paragraph vectors\n",
    "- Load all the trainign and test data into an array. This will create 4x12500 length arrays.\n",
    " - train/pos\n",
    " - train/neg\n",
    " - test/pos\n",
    " - test/neg\n",
    " each of these arays in fact includes a paragraph\n",
    "## Step 3: Cteate BOW\n",
    "- tokenize each element in each array. This shanges shape of the vectors. Initially vector shapes are (12500, 1). \n",
    "- do wordcount (word, count)\n",
    "- get rid of most common words\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/ubuntu/CyrusProjects/mxnet-notebooks/python/tutorials/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports and env-variable\n",
    "# imports and env-variable\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import urllib3\n",
    "import tarfile\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#BASE_DIR = '/home/ubuntu/CyrusProjects/mxnet-notebooks/python/tutorials'\n",
    "BASE_DIR = os.getcwd()\n",
    "PATH='/aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package hmm_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package hmm_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('pip3 install nltk')\n",
    "nltk.download('all')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "path = os.getcwd() + '/local_aclImdb_v1.tar.gz'\n",
    "chunk_size = 1024\n",
    "\n",
    "r = http.request('GET', url, preload_content=False)\n",
    "\n",
    "with open(path, 'wb') as out:\n",
    "    while True:\n",
    "        data = r.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        out.write(data)\n",
    "\n",
    "r.release_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89527\n"
     ]
    }
   ],
   "source": [
    "with open('aclImdb/imdb.vocab') as vocab_file:\n",
    "    vocab_array = vocab_file.readlines()\n",
    "vocab_array = [x.strip() for x in vocab_array]\n",
    "print(len(vocab_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89527\n",
      "89380\n"
     ]
    }
   ],
   "source": [
    "with open(BASE_DIR + PATH + '/imdb.vocab') as f:\n",
    "    dic = f.readlines()\n",
    "dic= [x.strip() for x in dic]\n",
    "dic = np.array(np.sort(dic))\n",
    "dic = np.unique(dic)\n",
    "dic = dic.tolist()\n",
    "filtered_dic = [word for word in dic if word not in stopwords.words('english')]\n",
    "print(len(dic))\n",
    "print(len(filtered_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sentence_array_creator(path, root_dir):\n",
    "    os.chdir(path)\n",
    "    sentences = []\n",
    "    for file in list(glob.glob(\"*.txt\")):\n",
    "        with open(file, 'r') as f:\n",
    "            sentences.append(f.readline().strip().lower())\n",
    "    return sentences\n",
    "    os.chdir(root_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method format of str object at 0x7fe844decbb8>\n",
      "12500, 12500, 12500, 12500\n"
     ]
    }
   ],
   "source": [
    "os.chdir(BASE_DIR)\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "base_dir = BASE_DIR + PATH\n",
    "pos_train_path = base_dir + '/train/pos'\n",
    "neg_train_path = base_dir + '/train/neg'\n",
    "pos_test_path = base_dir + '/test/pos'\n",
    "neg_test_path = base_dir + '/test/neg'\n",
    "\n",
    "\n",
    "pos_train_sentences = sentence_array_creator(pos_train_path, root_dir)\n",
    "neg_train_sentences = sentence_array_creator(neg_train_path, root_dir)\n",
    "pos_test_sentences = sentence_array_creator(pos_test_path, root_dir)\n",
    "neg_test_sentences = sentence_array_creator(neg_test_path, root_dir)\n",
    "print('{}; {}; {}; {}; '.format)\n",
    "\n",
    "print(\"{}, {}, {}, {}\".format(len(pos_train_sentences), len(neg_train_sentences), len(pos_test_sentences), len(neg_test_sentences)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39813\n"
     ]
    }
   ],
   "source": [
    "def get_dic_index(word, dic):\n",
    "    try:\n",
    "        ret_val = dic.index(word)\n",
    "    except ValueError: \n",
    "        ret_val = -1\n",
    "    return ret_val\n",
    "\n",
    "    \n",
    "word = \"is\"\n",
    "print(get_dic_index(word, dic))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sentence_encoder(sentences, vocab):\n",
    "    bow = []\n",
    "    t = RegexpTokenizer(r'\\w+')\n",
    "    prog = 0\n",
    "    for sentence in sentences:\n",
    "        if prog % 10 == 0:\n",
    "            print(prog, sep=' ', end='-',flush=True)\n",
    "        s = ''.join(sentence)\n",
    "        tokens = t.tokenize(s)\n",
    "        indexes = []\n",
    "        for token in tokens:\n",
    "            idx = get_dic_index(token, vocab)\n",
    "            if idx != -1:\n",
    "                indexes.append(idx)\n",
    "        prog += 1\n",
    "        bow.append(indexes)\n",
    "    return bow\n",
    "\n",
    "def sentence_tokenizer(sentences):\n",
    "    bow = []\n",
    "    t = RegexpTokenizer(r'\\w+')\n",
    "    for sentence in sentences:\n",
    "        s = ''.join(sentence)\n",
    "        tokens = t.tokenize(s)\n",
    "        bow.append(tokens)\n",
    "    return bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_items(array):\n",
    "    for i in array:\n",
    "        print('{} \\n'.format(i))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now calculating bow_pos_train_sentences...\n",
      "0-10-20-30-40-50-60-70-80-90-100-110-120-130-140-150-160-170-180-190-200-210-220-230-240-250-260-270-280-290-300-310-320-330-340-350-360-370-380-390-400-410-420-430-440-450-460-470-480-490-500-510-520-530-540-550-560-570-580-590-600-610-620-630-640-650-660-670-680-690-700-710-720-730-740-750-760-770-780-790-800-810-820-830-840-850-860-870-880-890-900-910-920-930-940-950-960-970-980-990-1000-1010-1020-1030-1040-1050-1060-1070-1080-1090-1100-1110-1120-1130-1140-1150-1160-1170-1180-1190-1200-1210-1220-1230-1240-1250-1260-1270-1280-1290-1300-1310-1320-1330-1340-1350-1360-1370-1380-1390-1400-1410-1420-1430-1440-1450-1460-1470-1480-1490-1500-1510-1520-1530-1540-1550-1560-1570-1580-1590-"
     ]
    }
   ],
   "source": [
    "print(\"now calculating bow_pos_train_sentences...\")\n",
    "bow_pos_train_sentences = sentence_encoder(pos_train_sentences, filtered_dic)\n",
    "np.save(BASE_DIR + PATH + '/bow_pos_train_sentences_batch', bow_pos_train_sentences)\n",
    "\n",
    "print(\"now calculating bow_neg_train_sentences...\")\n",
    "bow_neg_train_sentences = sentence_encoder(neg_train_sentences, filtered_dic)\n",
    "np.save(BASE_DIR + PATH + '/bow_neg_train_sentences_batch', bow_neg_train_sentences)\n",
    "\n",
    "print(\"now calculating bow_pos_test_sentences...\")\n",
    "bow_pos_test_sentences = sentence_encoder(pos_test_sentences, filtered_dic)\n",
    "np.save(BASE_DIR + PATH + '/bow_pos_test_sentences_batch', bow_pos_test_sentences)\n",
    "\n",
    "print(\"now calculating bow_neg_test_sentences...\")\n",
    "bow_neg_test_sentences = sentence_encoder(neg_test_sentences, filtered_dic)\n",
    "np.save(BASE_DIR + PATH + '/bow_neg_test_sentences_batch', bow_neg_test_sentences)\n",
    "\n",
    "print(\"DONE...\")\n",
    "\n",
    "print(len(bow_neg_test_sentences))\n",
    "print(len(bow_neg_train_sentences))\n",
    "print(len(bow_pos_test_sentences))\n",
    "print(len(bow_pos_train_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500, 12500, 12500, 12500\n"
     ]
    }
   ],
   "source": [
    "a = np.load(BASE_DIR + PATH + '/bow_pos_train_sentences_batch.npy')\n",
    "b = np.load(BASE_DIR + PATH + '/bow_neg_train_sentences_batch.npy')\n",
    "c = np.load(BASE_DIR + PATH + '/bow_pos_test_sentences_batch.npy')\n",
    "d = np.load(BASE_DIR + PATH + '/bow_neg_test_sentences_batch.npy')\n",
    "\n",
    "print('{}, {}, {}, {}'.format(len(a), len(b), len(c), len(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   31     7]\n",
      " [  253     1]\n",
      " [  888     1]\n",
      " [ 1211     1]\n",
      " [ 1252     1]\n",
      " [ 1713     1]\n",
      " [ 1830     2]\n",
      " [ 2027     2]\n",
      " [ 2108     2]\n",
      " [ 2195     1]\n",
      " [ 2518     1]\n",
      " [ 2614    14]\n",
      " [ 3281     1]\n",
      " [ 3292     1]\n",
      " [ 3294     1]\n",
      " [ 3554     1]\n",
      " [ 3728     2]\n",
      " [ 3837     1]\n",
      " [ 3917     1]\n",
      " [ 4108     2]\n",
      " [ 4424     1]\n",
      " [ 4983     1]\n",
      " [ 5255     1]\n",
      " [ 5918     1]\n",
      " [ 6333     1]\n",
      " [ 6413     1]\n",
      " [ 6474     1]\n",
      " [ 6576     1]\n",
      " [ 6617     1]\n",
      " [ 7157     1]\n",
      " [ 7504     1]\n",
      " [ 9132     1]\n",
      " [ 9147    20]\n",
      " [ 9973     1]\n",
      " [10451     1]\n",
      " [10492     8]\n",
      " [10589     1]\n",
      " [11048     1]\n",
      " [11308     1]\n",
      " [11377     2]\n",
      " [11958     1]\n",
      " [14735     1]\n",
      " [14873     1]\n",
      " [14997     1]\n",
      " [15090     1]\n",
      " [15480     1]\n",
      " [15793     1]\n",
      " [16902     1]\n",
      " [17676     1]\n",
      " [17868     1]\n",
      " [17898     3]\n",
      " [18256     1]\n",
      " [18257     1]\n",
      " [18421     1]\n",
      " [18962     1]\n",
      " [20233     1]\n",
      " [20651     1]\n",
      " [20778     1]\n",
      " [21482     1]\n",
      " [21486     2]\n",
      " [21648     1]\n",
      " [23299     1]\n",
      " [23899     1]\n",
      " [24282     1]\n",
      " [24750     1]\n",
      " [25003     1]\n",
      " [25171     1]\n",
      " [25309     2]\n",
      " [25423     1]\n",
      " [25653     1]\n",
      " [26565     1]\n",
      " [26959     2]\n",
      " [27772     1]\n",
      " [27794     2]\n",
      " [27814     1]\n",
      " [27946     1]\n",
      " [28014     1]\n",
      " [28777     4]\n",
      " [29029     1]\n",
      " [29092     2]\n",
      " [29587     1]\n",
      " [29945     1]\n",
      " [31012     1]\n",
      " [31391     1]\n",
      " [31601     1]\n",
      " [31772     2]\n",
      " [32139     2]\n",
      " [32726     1]\n",
      " [33089     1]\n",
      " [33288     2]\n",
      " [34348     1]\n",
      " [34364     1]\n",
      " [34482     2]\n",
      " [34586     7]\n",
      " [35037     1]\n",
      " [35143     6]\n",
      " [35291     1]\n",
      " [35641     2]\n",
      " [35753     4]\n",
      " [36370     1]\n",
      " [36373     1]\n",
      " [36737     1]\n",
      " [37329     6]\n",
      " [37535     2]\n",
      " [38082     6]\n",
      " [38280     1]\n",
      " [38749     1]\n",
      " [38849     1]\n",
      " [39038     1]\n",
      " [39454     1]\n",
      " [39460     1]\n",
      " [39560     2]\n",
      " [39813    12]\n",
      " [39920     7]\n",
      " [40767     1]\n",
      " [42054     1]\n",
      " [42457     1]\n",
      " [45010     2]\n",
      " [45599     5]\n",
      " [46022     1]\n",
      " [47691     1]\n",
      " [47884     1]\n",
      " [47893     1]\n",
      " [48180     2]\n",
      " [49020     1]\n",
      " [50280     2]\n",
      " [50286     1]\n",
      " [50331     2]\n",
      " [50648     5]\n",
      " [51028     1]\n",
      " [51191     2]\n",
      " [51643     1]\n",
      " [51737     1]\n",
      " [51962     1]\n",
      " [52191     1]\n",
      " [52720     1]\n",
      " [52891     1]\n",
      " [53200     2]\n",
      " [53874     2]\n",
      " [53972     1]\n",
      " [53990     1]\n",
      " [53997     1]\n",
      " [54754    12]\n",
      " [54762     3]\n",
      " [55357     2]\n",
      " [56109     1]\n",
      " [56830     1]\n",
      " [57184     1]\n",
      " [58069     2]\n",
      " [58153     1]\n",
      " [58194     1]\n",
      " [58477     1]\n",
      " [59370     1]\n",
      " [59869     1]\n",
      " [60485     2]\n",
      " [62858     1]\n",
      " [62897     1]\n",
      " [63499     1]\n",
      " [63969     1]\n",
      " [64018     1]\n",
      " [64024     6]\n",
      " [64610     1]\n",
      " [64872     1]\n",
      " [65014     1]\n",
      " [66645     1]\n",
      " [66702     1]\n",
      " [67435    10]\n",
      " [67630     1]\n",
      " [68119     1]\n",
      " [68276     1]\n",
      " [68715     1]\n",
      " [69287     2]\n",
      " [69319     1]\n",
      " [70435     1]\n",
      " [70612    12]\n",
      " [71105     1]\n",
      " [71162     1]\n",
      " [71380     1]\n",
      " [71466     5]\n",
      " [71511     1]\n",
      " [71668     1]\n",
      " [72991     2]\n",
      " [73069     1]\n",
      " [73342     1]\n",
      " [74218     1]\n",
      " [74739     1]\n",
      " [74871     1]\n",
      " [75191     1]\n",
      " [75241     1]\n",
      " [75432     1]\n",
      " [75618     1]\n",
      " [75762     1]\n",
      " [76370     2]\n",
      " [76496     1]\n",
      " [77507     1]\n",
      " [77642     8]\n",
      " [77814     1]\n",
      " [78568     1]\n",
      " [78593     1]\n",
      " [78977     5]\n",
      " [78983     1]\n",
      " [79005    10]\n",
      " [79025    15]\n",
      " [79093     1]\n",
      " [79158     3]\n",
      " [79205     1]\n",
      " [79264     1]\n",
      " [79275     1]\n",
      " [79280     1]\n",
      " [79283     2]\n",
      " [79344     5]\n",
      " [79989     1]\n",
      " [80034     1]\n",
      " [80083    10]\n",
      " [80321     1]\n",
      " [81313     1]\n",
      " [81548     1]\n",
      " [81618     1]\n",
      " [81754     1]\n",
      " [84004     1]\n",
      " [84080     1]\n",
      " [84689     3]\n",
      " [84937     1]\n",
      " [86004     1]\n",
      " [86154     1]\n",
      " [86187     1]\n",
      " [86370     2]\n",
      " [86924     2]\n",
      " [87003     3]\n",
      " [87007     1]\n",
      " [87114     1]\n",
      " [87917     1]\n",
      " [88060     1]\n",
      " [88074     1]\n",
      " [88106     2]\n",
      " [88193     1]\n",
      " [88207     2]\n",
      " [88785     1]\n",
      " [88794     2]\n",
      " [88923     4]]\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "for item in a:\n",
    "    a_cnt = (np.bincount(item))\n",
    "    # print('{}; {}; {}; {}; {}'.format(a_cnt, len(item), len(a_cnt), a_cnt.mean(), a_cnt.var()))\n",
    "k = [95, 23, 123, 95, 23, 456, 222, 95, 123, 95, 123, 657]\n",
    "#print(np.bincount(k)\n",
    "\n",
    "\n",
    "x = a[100]\n",
    "y = np.bincount(x)\n",
    "ii = np.nonzero(y)[0]    \n",
    "zip(ii,y[ii]) \n",
    "print(np.vstack((ii,y[ii])).T)\n",
    "print(dic[80083])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89527\n",
      "89380\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600 in 140033611142912 \n",
      "4600 in 140033602750208 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_bag_of_words(input_para):\n",
    "    output_para = []\n",
    "    \n",
    "    \n",
    "    return output_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StringIO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2f9fc64d9ffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"M 21 72\\nF 35 58\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'formats'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'S1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StringIO' is not defined"
     ]
    }
   ],
   "source": [
    "d = StringIO(\"M 21 72\\nF 35 58\")\n",
    "np.loadtxt(d, dtype={'names': ('gender', 'age', 'weight'),'formats': ('S1', 'i4', 'f4')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'lstm_bucketing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-acdfb6615548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlstm_bucketing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'lstm_bucketing'"
     ]
    }
   ],
   "source": [
    "import lstm_bucketing\n",
    "\n",
    "\n",
    "    train_sent, vocab = tokenize_text(\"./data/ptb.train.txt\", start_label=start_label,\n",
    "                                      invalid_label=invalid_label)\n",
    "    val_sent, _ = tokenize_text(\"./data/ptb.test.txt\", vocab=vocab, start_label=start_label,\n",
    "                                invalid_label=invalid_label)\n",
    "\n",
    "    data_train  = mx.rnn.BucketSentenceIter(train_sent, args.batch_size, buckets=buckets,\n",
    "                                            invalid_label=invalid_label)\n",
    "    data_val    = mx.rnn.BucketSentenceIter(val_sent, args.batch_size, buckets=buckets,\n",
    "                                            invalid_label=invalid_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
